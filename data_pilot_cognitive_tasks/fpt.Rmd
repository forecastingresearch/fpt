---
title: "FPT Cognitive Tasks - Data Processing Pipeline - Pilot data"
author: "Nikolay Petrov"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    number_sections: true
    code_folding: hide
params:
  LOAD_DATA_FROM_CACHE: TRUE
  RENDER_STOP_AFTER_CACHE: FALSE
---

# Setup

```{r include=FALSE}
knitr::opts_chunk$set(eval=TRUE, include=TRUE, echo=TRUE, message=FALSE, warning=FALSE, fig.width=12, fig.height=9)
```

```{r}
dirs <- c("cache", "convenience_datasets", "task_datasets")
for (d in dirs) {
  if (!dir.exists(d)) {
    dir.create(d, showWarnings = FALSE, recursive = TRUE)
  }
}
```

```{r}
show_data_markdown <- function(data, group_var=NULL, rows_by_group=Inf, row_limit=Inf) {
  if (!is.null(group_var)) {
    data_to_display <- data %>%
      group_by(.data[[group_var]])
  } else {
    data_to_display <- data
  }
  data_to_display <- data_to_display %>%
    filter(row_number()<=rows_by_group) %>%
    ungroup() %>%
    filter(row_number()<=row_limit)
  
  if (isTRUE(getOption('knitr.in.progress'))) {
    return(knitr::kable(data_to_display))
  } else {
    return(data_to_display)
  }
}
```

```{r}
# Custom formatting function for APA style
format_apa <- function(x, is_p_val=F) {
  rounding_digits <- 2
  
  if (is_p_val) {
    if (x<0.001) {
      return ("< .001")
    }
    rounding_digits <- 3
  }
  
  sign <- ifelse(x < 0, "-", "")
  formatted <- formatC(abs(x), format = "f", digits = rounding_digits)
  formatted <- gsub("^0", "", formatted) # Remove leading zero
  formatted <- paste0(sign, formatted)
  
  if (is_p_val) {
    return(paste0("= ", formatted))
  }
  return(formatted)
}

form_labels <- c("33ff78"="Form 1", "223301"="Form 2")
```

```{r}
## Deduplicate task records across participant waves
# Keeps first occurrence per subject/wave combination based on specified columns
# Removes subject_id column before returning cleaned data
remove_duplicated_tasks_across_waves <- function(data, ...) {
    group_cols <- rlang::quos(...)
    required_cols <- c("subject_id", "wave")
    
    # Determine grouping columns for distinct operation
    grouping_cols <- if (length(group_cols) == 0) {
        "subject_id"
    } else {
        c("subject_id", purrr::map_chr(group_cols, rlang::quo_text))
    }
    
    # Add missing metadata if needed
    data <- data %>%
        left_join(
            metadata_session %>% 
                select(session_id, any_of(required_cols)),
            by = "session_id"
        )
    
    # Remove duplicates keeping first entry per subject/wave
    data %>%
        arrange(subject_id, wave) %>%
        distinct(!!!rlang::syms(grouping_cols), .keep_all = TRUE) %>%
        select(-subject_id)
}
```

Packages loading and setup:

```{r warning=FALSE, message=FALSE}
options(dplyr.summarise.inform = FALSE)

packages <- c(
              "tidyverse",
              "jsonlite",
              "scales", ## rescale function
              "officer"
              )

if (!all(packages %in% (.packages()))) {
  #using invisible to hide output
  invisible(lapply(packages,
         FUN = function(x) {
           if (!require(x, character.only = TRUE)) {
             install.packages(x, dependencies = TRUE)
           }
           library(x, character.only = TRUE)
         }))
}

theme_set(theme_classic())
theme_update(plot.title = element_text(size=25, hjust=0.5),
             plot.subtitle = element_text(size=18, hjust=0.5, face="italic"),
            axis.title = element_text(size=15),
            axis.text = element_text(size=10),
            legend.title = element_text(size=20),
            legend.text = element_text(size=15),
            legend.position = "bottom",
            legend.justification="right")
```

Columns and their types per task:

```{r}
col_spec_parameters <- list(
  experiment_parameters='c', task_leapfrog='c', task_admc_rc1='c', task_admc_a1='c', task_admc_rc2='c', task_admc_a2='c',
  task_admc_dr='c', task_admc_rp='c', task_denominator_neglect_version_A='c', task_denominator_neglect_version_B='c',
  task_graph_literacy='c', task_impossible_question='c', task_time_series='c', task_bayesian_update_easy='c',
  task_bayesian_update_hard='c', task_cognitive_reflection='c', task_berlin_numeracy='c', task_number_series='c',
  task_coherence_forecasting='c', task_raven_matrix='c', task_conditional_forecasting='c', task_shipley_vocab='c',
  task_shipley_abstraction='c'
)

col_spec_session_info <- list(
  FULLSCREEN_SCREEN_WIDTH='i', FULLSCREEN_SCREEN_HEIGHT='i', width='i', height='i', webaudio='l', browser='c',
  browser_version='c', mobile='l', os='f', fullscreen='l', vsync_rate='d', webcam='l', microphone='l'
)

col_spec_across_tasks <- list(
  session_id='f', session_restart_id='f',
  # subject_id = 'f', form = 'f', wave = 'f',
  time_elapsed = 'i', trial_index = 'i',custom_timer_ended_trial = 'l',
  trial_name = 'f', block = 'd', trial = 'd', pt_trial = 'l',
  response = 'c', rt = 'd', trial_type='f'
)

col_spec_admc <- list(
  admc_id = 'c', admc_response = 'c'
)

col_spec_leapfrog <- list(
  curr_block_ind='d', pt_trial_prediction ='d',
  optionA_reward = 'd', optionB_reward = 'd',
  points_won = 'd', option_selected = 'f'
)

col_spec_denominator_neglect <- list(
  task_version='f', choice_type='f',
  left_lottery_display_type='f', right_lottery_display_type='f',
  left_lottery_type='f', right_lottery_type='f',
  left_lottery_gold_prop='d', right_lottery_gold_prop='d',
  left_lottery_total_coins='d', right_lottery_total_coins='d',
  left_lottery_gold_coins='d', right_lottery_gold_coins='d',
  left_lottery_silver_coins='d', right_lottery_silver_coins='d',
  selected_lottery='f', coin_drawn='f'
)

col_spec_graph_literacy <- list(
  Q1='d', Q2='d', Q3='d', Q4='d', Q5='d', Q6='d', Q7='d', Q8='d', Q9='d',
  Q10='d', Q11='d', Q12='d', Q13='d'
)

col_spec_impossible_questions <- list(
  id='f',
  question_text='c', answer_1='c', answer_2='c',
  correct_answer='d', aig_version='f', correct='l', 
  response_slider='d', response_choice='d'
)

col_spec_time_series <- list(
  func='f', direction='f', noise_percent='d', noise_condition='f',
  y_axis_values='c', chart_height='d', datapoints='f'
)

col_spec_bayesian_update <- list(
  unique_trial='d', unique_trial_draw_number='d',
  left_box_majority_color='f', right_box_majority_color='f', selected_box_majority_color='f',
  ball_split='c', version='f', current_draw='c', past_draws='c',
  response_slider='d',
  score='d'
)

col_spec_cognitive_reflection <- list(
  crt_1='c', crt_2='c', crt_3='c', crt_4='c', crt_5='c', crt_6='c', crt_7='c'
)

col_spec_number_series <- list(
  ns_id='f', ns_response='c'
)

col_spec_coherence_forecasting <- list(
  cfs_id='c', cfs_response='c'
)

col_spec_raven <- list(
  stimulus='c',
  correct_answer='d', correct='l', list_id='d'
)

col_spec_conditional_forecasting <- list(
  cond_forecast_id='c', cond_forecast_response='c'
)

col_spec_berlin_numeracy <- list(
  BNS_1='c', BNS_2='c', BNS_3='c', BNS_4='c'
)

col_spec_misc <- list(
  data_checkpoint='f', data_checkpoint_ind='i', view_history='c', success='l',
  subject_id = 'f', form = 'f', wave = 'f'
)

col_spec_data <- c(
  col_spec_parameters,
  col_spec_session_info,
  col_spec_across_tasks,
  col_spec_admc,
  col_spec_leapfrog,
  col_spec_denominator_neglect,
  col_spec_graph_literacy,
  col_spec_impossible_questions,
  col_spec_time_series,
  col_spec_bayesian_update,
  col_spec_cognitive_reflection,
  col_spec_number_series,
  col_spec_coherence_forecasting,
  col_spec_raven,
  col_spec_conditional_forecasting,
  col_spec_berlin_numeracy,
  col_spec_misc
)

col_spec_data <- do.call(cols, col_spec_data)
```

# `metadata_tables`

```{r}
col_spec_metadata_session <- cols(
  session_id='f', completed='l', subject_id='f', form='f', wave='f', 
  task_order_indices='c',
  added_at = col_datetime(format = "%Y-%m-%d %H:%M:%S"), last_accessed = col_datetime(format = "%Y-%m-%d %H:%M:%S")
)
col_spec_metadata_session_restarts <- cols(
  session_id='f', session_restart_id='f',
  data_checkpoint_ind='d', ms_since_data_checkpoint_ind='d', trials_completed_since_data_checkpoint_ind='d',
  added_at = col_datetime(format = "%Y-%m-%d %H:%M:%S"), modified_at = col_datetime(format = "%Y-%m-%d %H:%M:%S")
)
col_spec_data_checkpoints <- cols(
  session_id='f', 
  data_checkpoint='f', data_checkpoint_ind='d',
  added_at = col_datetime(format = "%Y-%m-%d %H:%M:%S")
)
col_spec_incentive_condition <- cols(
  subject_id='f',
  incentive_condition='f'
)

col_spec_recruitment_condition <- cols(
  subject_id='f',
  recruitment_invited_to_wave0='l'
)
```

```{r}
metadata_session <- read_csv(file.path("metadata_tables", "session.csv"), col_types=col_spec_metadata_session)
metadata_session_restarts <- read_csv(file.path("metadata_tables", "session_restarts.csv"), col_types=col_spec_metadata_session_restarts)
metadata_data_checkpoints <- read_csv(file.path("metadata_tables", "data_checkpoints.csv"), col_types=col_spec_data_checkpoints)
metadata_incentive_condition <- read_csv(file.path("metadata_tables", "incentive_condition.csv"), col_types=col_spec_incentive_condition)
metadata_recruitment_condition <- read_csv(file.path("metadata_tables", "recruitment_condition.csv"), col_types=col_spec_recruitment_condition)
```

# `data` processing

This is a table that contains all data across all tasks and sessions.

```{r}
if (params$LOAD_DATA_FROM_CACHE) {
  load(file.path("cache", "task_parameters_data.RData"))
  load(file.path("cache", "session_info.RData"))
  load(file.path("metadata_tables", "task_aig_version.RData"))
  load(file.path("cache", "data_processed.RData"))
}
```

## Reading in all data

Reading in all data:

```{r data-read-heavy, warning=FALSE, message=FALSE}
if (!params$LOAD_DATA_FROM_CACHE) {
  get_data <- function(datasets_files_list, datapath) {
    total_files <- length(datasets_files_list)
    start_time <- Sys.time()
    
    # a bit of a hacky solution to init as a list
    # and then bind_rows into a tibble
    # but helps reduce memory overhead and having to clean-up the list
    data <- vector("list", length(datasets_files_list))
    for (i in seq_along(datasets_files_list)) {
      if (i %% ceiling(total_files / 20) == 0 || i == total_files) {
        elapsed_time <- difftime(Sys.time(), start_time, units = "secs")
        cat(sprintf("Progress: %d%%, File: %d/%d, Elapsed: %.2f seconds\n", 
                    round(i / total_files * 100), i, total_files, elapsed_time))
        flush.console() # prints immediately
      }
      
      data[[i]] <- read_csv(file.path(datapath, datasets_files_list[i]), col_types = col_spec_data) %>%
        select(any_of(names(col_spec_data$cols))) %>%
        mutate(stimulus = if_else(trial_name=="raven_trial", stimulus, NA_character_))
    }
    data <- bind_rows(data)
    
    return(data)
  }
    
  datapath <- "raw_session_data"
  datasets_files_list <- list.files(path=datapath, pattern="*.csv")
  
  data <- get_data(datasets_files_list, datapath)
  save(data, file=file.path("cache", "data_raw.RData"))
}
```

## Session info and task parameters

Task-specific parameters are removed from `data` as they are quite large and are kept in `task_parameters_data`.

Ditto for session info (os, browser, screen width) - kept in `session_info`

```{r create-task-params-and-session-info}
if (!params$LOAD_DATA_FROM_CACHE) {
  task_parameters_data <- data %>%
    select(session_id, any_of(names(col_spec_parameters))) %>%
    filter(!is.na(experiment_parameters))
  save(task_parameters_data, file=file.path("cache", "task_parameters_data.RData"))
    
    # remove from data
  data <- data %>%
    select(-colnames(select(task_parameters_data, -session_id)))
  
  session_info <- data %>%
    select(session_id, any_of(names(col_spec_session_info))) %>%
    filter(!is.na(width))
  save(session_info, file=file.path("cache", "session_info.RData"))
  
  # remove from data
  data <- data %>%
    select(-colnames(select(session_info, -session_id)))
  
  # remove misc cols from data
  data <- data %>%
    select(-any_of(names(col_spec_misc)))
}
```

## Creating AIG/Anchor lookup table

While information was kept as to which form a participant completed, there is no easily accessible table saved to look up which task version (AIG vs anchor) a participant completed within a session. We extract that information here and create such table.

```{r create-metadata-task-aig-version-dataset}
if (!params$LOAD_DATA_FROM_CACHE) {
  tmp <- c("task_denominator_neglect_version_A", "task_denominator_neglect_version_B", "task_time_series", "task_leapfrog",
           "task_impossible_question", "task_bayesian_update_easy", "task_bayesian_update_hard")

  metadata_task_aig_version <- task_parameters_data %>%
    select(session_id, all_of(tmp)) %>%
    mutate(
      across(
        all_of(tmp),
        ~ case_when(
            grepl("'USE_ANCHOR_VERSION': False", .) ~ "AIG",
            grepl("'USE_ANCHOR_VERSION': True", .) ~ "anchor",
            TRUE ~ NA_character_  # Default to NA if neither pattern is found
          )
      )
    ) %>%
    pivot_longer(cols=-session_id, names_to="task", names_transform = list(task = ~ sub("^task_", "", .)), values_to="AIG_version")
  
  save(metadata_task_aig_version, file=file.path("metadata_tables", "task_aig_version.RData"))
  write.csv(metadata_task_aig_version, file.path("metadata_tables", "task_aig_version.csv"), row.names = FALSE)
}
```

## Updating `time_elapsed` and `trial_index` due to restarts

As people can close the current session and come back to it later, this is reflected in `time_elapsed` and `trial_index` (also in `internal_node_id` but we don't use that one). Basically, when the browser window is closed and the PP comes back to the session later, both `time_elapsed` and `trial_index` restart from 0. We will update both these variables to make it seem like it was a single session - for ease of analysis.


```{r update-time-elapsed-trial-index}
if (!params$LOAD_DATA_FROM_CACHE) {
  update_data_time_elapsed_and_trial_index <- function(data) {
    df_list_by_session <- data %>%
      group_by(session_id) %>%
      group_split()
    
    total_sessions <- length(df_list_by_session)
    start_time <- Sys.time()
    
    adjusted_dfs <- lapply(seq_along(df_list_by_session), function(idx) {
      if (idx %% ceiling(total_sessions / 20) == 0 || idx == total_sessions) {
        elapsed_time <- difftime(Sys.time(), start_time, units = "secs")
        cat(sprintf("Progress: %d%%, Session: %d/%d, Elapsed: %.2f seconds\n", 
                    round(idx / total_sessions * 100), idx, total_sessions, elapsed_time))
        flush.console()
      }
      
      df_group <- df_list_by_session[[idx]]
      
      # another way to flag this is by session_restart_id
      df_group <- df_group %>%
        mutate(session_flag = cumsum(trial_index == 0))
      
      df_list <- split(df_group, df_group$session_flag)
      
      if (length(df_list) > 1) {
        for (i in 2:length(df_list)) {
          last_trial_index <- max(df_list[[i-1]]$trial_index)
          last_time_elapsed <- max(df_list[[i-1]]$time_elapsed)
          
          df_list[[i]]$trial_index <- df_list[[i]]$trial_index + last_trial_index + 1
          df_list[[i]]$time_elapsed <- df_list[[i]]$time_elapsed + last_time_elapsed
        }
      }
      
      bind_rows(df_list) %>%
        select(-session_flag)
    })
    
    return(bind_rows(adjusted_dfs))
  }
  
  data <- update_data_time_elapsed_and_trial_index(data)
  save(data, file=file.path("cache", "data_processed.RData"))
}
```

```{r}
if (params$RENDER_STOP_AFTER_CACHE) {
  knitr::knit_exit()
}
```

## Filtering out non-completers

```{r}
completes_single_session <- metadata_session %>%
  group_by(subject_id, wave) %>%
  filter(n() == 1, completed==1)

completes_multiple_sessions_first_attempt <- metadata_session %>%
  group_by(subject_id, wave) %>%
  filter(n() > 1, completed==1) %>%
  ungroup() %>%
  arrange(subject_id, added_at) %>%
  distinct(subject_id, wave, .keep_all = TRUE)

final_completers <- bind_rows(completes_single_session, completes_multiple_sessions_first_attempt)

write.csv(final_completers, file.path("metadata_tables", "final_completers.csv"), row.names = FALSE)
```

```{r}
data <- data %>%
  filter(session_id %in% final_completers$session_id)
```

# Task-specific datasets

## Leapfrog

### Data

Interblock question data during practice trials:

```{r}
data_leapfrog_pt_trials_interblock <- data %>%
  filter(trial_name=="leapfrog_pt_trial_interblock_question") %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_leapfrog)))) %>%
  select(session_id, curr_block_ind, pt_trial_prediction)
```

```{r}
data_leapfrog <- data %>%
  filter(trial_name=="leapfrog_trial", pt_trial==F) %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_leapfrog)))) %>%
  select(-c(curr_block_ind, pt_trial_prediction, trial_name, pt_trial, custom_timer_ended_trial))
```

### Scoring

```{r}
data_leapfrog <- data_leapfrog %>%
  mutate(optimal_choice = as.integer(points_won == pmax(optionA_reward, optionB_reward)))
```

## Denominator neglect

### Data

Trials must have an "id". Given that randomization was done `choice_type` x `small_lottery_gold_prop` x `large_lottery_gold_prop_diff` but order of these 48 trials was randomized between sessions, we add a `trial_id` here, which uniquely identifies the same trial across sessions. Recall that `trial` is just the ordering within session and trial_index is the experiment-wide `jsPsych`-generated trial counter.

Also, when working with the `prop`-related variables, rounding is added as this sometimes leads to weirdness!

```{r}
data_denominator_neglect <- data %>%
  # RT timings in raw data were sometimes incorrect due to a technical error
  # (some timers were not being cleared properly at end of some trials so the 
  # timings merged between trials or registered on incorrect trials)
  # after a careful examination, we determined that we can replace those with
  # RTs based on time_elapsed
  # the differences were in the sub-10ms range
  group_by(session_id) %>%
  mutate(rt = time_elapsed-lag(time_elapsed)) %>%
  ungroup() %>%
  filter(startsWith(as.character(trial_name), "dn_main_trial"), pt_trial==F) %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_denominator_neglect)))) %>%
  # choice_type was incorrectly defined for some trials in anchor versions
  # so we fix it here
  mutate(choice_type = case_when(
    left_lottery_type=="small" & left_lottery_gold_prop < right_lottery_gold_prop ~ "harmony",
    left_lottery_type=="small" & left_lottery_gold_prop > right_lottery_gold_prop ~ "conflict",
    right_lottery_type=="small" & left_lottery_gold_prop > right_lottery_gold_prop ~ "harmony",
    right_lottery_type=="small" & left_lottery_gold_prop < right_lottery_gold_prop ~ "conflict",
    TRUE ~ NA_character_
    ),
    choice_type = as.factor(choice_type)) %>%
  # technically trial_name should have varied based on version but it doesn't
  # task_version contains info about the task version
  select(-c(trial_name, pt_trial, block)) %>%
  # add some more info that should have been saved
  mutate(small_lottery_gold_prop = round(if_else(left_lottery_type=="small", left_lottery_gold_prop, right_lottery_gold_prop), 2),
         large_lottery_gold_prop_diff = round(abs(left_lottery_gold_prop-right_lottery_gold_prop), 2)) %>%
  # add trial_id
  left_join(tibble(
    trial_id = 1:48,
    choice_type = rep(c("conflict", "harmony"), each=24),
    small_lottery_gold_prop = round(rep(rep(c(0.1, 0.2, 0.3), each=8), 2), 2),
    large_lottery_gold_prop_diff = round(rep(seq(0.01, 0.08, 0.01), 6), 2)
  ), by=c("choice_type", "small_lottery_gold_prop", "large_lottery_gold_prop_diff"))
```

### Scoring

Take the first one

```{r}
data_denominator_neglect <- data_denominator_neglect %>% 
  mutate(correct = as.integer(
      (left_lottery_gold_prop > right_lottery_gold_prop & selected_lottery == "left") |
      (left_lottery_gold_prop < right_lottery_gold_prop & selected_lottery == "right")
    )
  )
```

## Graph literacy

### Data

```{r}
data_graph_literacy_raw <- data %>%
  filter(trial_name=="gl_main_trial") %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_graph_literacy)))) %>%
  remove_duplicated_tasks_across_waves() %>%
  select(session_id, session_restart_id, time_elapsed, trial_index, custom_timer_ended_trial, rt, Q1:Q13)
```

### Scoring

```{r}
data_graph_literacy_scores <- data_graph_literacy_raw %>%
  select(session_id, Q1:Q13) %>%
  pivot_longer(cols=-c(session_id),
               names_to="question",
               values_to="response",
               names_transform = list(question = ~str_remove(., "Q"))) %>%
  mutate(question=as.integer(question)) %>%
  # correct answers
  left_join(tibble(
    question = seq(1, 13),
    correct_answer = c(35, 15, 25, 25, 20, 3, 25, 40, 20, 3, 2, 2, 5)
  ), by="question") %>%
  mutate(correct = as.integer(response==correct_answer))
```

## Impossible question

### Data

```{r}
data_impossible_question <- data %>%
  filter(trial_name=="iqc_test_trial") %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_impossible_questions)))) %>%
  select(-c(pt_trial, trial_name, block, trial_type)) %>%
  mutate(question_type = if_else(str_detect(id, "^IQ_"), "IQ", "GK"),
         confidence_scaled = (response_slider-50) / (100-50))
```

### Scoring

The task comes pre-scored -- `correct` column contains info on whether the response was correct or not.

```{r}
data_impossible_question <- data_impossible_question %>%
  mutate(correct=as.integer(correct))
```

## Time series

### Data

```{r}
# custom as.numeric equivalent function; keeps warnings if warranted
# https://stackoverflow.com/a/36239701/13078832
as.num = function(x, na.strings = "NA") {
  stopifnot(is.character(x))
  na = x %in% na.strings
  x[na] = "0"
  x = as.numeric(x)
  x[na] = NA_real_
  x
}

data_time_series <- data %>%
  filter(trial_name=="time_series_trial", pt_trial==F) %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_time_series)))) %>%
  select(-c(trial_name, pt_trial, block, trial_type, response)) %>%
  # parsing the y_axis_values
  mutate(y_axis_values = str_replace_all(y_axis_values, "\\[|\\]|'", "") %>%
           str_split(",") %>%
           map(~as.num(.x, " None")),
         y_axis_values = map(y_axis_values, ~.x[seq_len(36)])) %>%
  mutate(displayed_values = if_else(datapoints == "datapoints_10",
                                    map(y_axis_values, ~.x[1:10]),
                                    map(y_axis_values, ~.x[1:30])),
         prediction = if_else(datapoints == "datapoints_10",
                                map(y_axis_values, ~.x[11:16]),
                                map(y_axis_values, ~.x[31:36]))) %>%
  unnest_wider(prediction, names_sep="_") %>%
  # scale predictions relative to screen size
  mutate(across(starts_with("prediction_"), ~ . / chart_height))
```

### Scoring

Y axis values are scaled to [0-1]. MSE is reported per-trial.

```{r}
time_series_ground_truth <- tibble(
  func = c("exponential", "linear", "linear", "exponential", "exponential", "linear", "exponential", "linear"),
  direction = c("positive", "negative", "positive", "negative", "positive", "positive", "negative", "negative"),
  datapoints = c("datapoints_30", "datapoints_30", "datapoints_10", "datapoints_10", "datapoints_10", "datapoints_30", "datapoints_30", "datapoints_10"),
  ground_truth_prediction = list(
    c(0.45545032182429973, 0.5080480511704113, 0.5702256895001414, 0.6437280774610872, 0.7306178529670532, 0.8333333333333326),
    c(0.3796296296296296, 0.37037037037037035, 0.3611111111111111, 0.3518518518518518, 0.34259259259259256, 0.33333333333333326),
    c(0.5624999999999999, 0.5833333333333334, 0.6041666666666667, 0.625, 0.6458333333333333, 0.6666666666666666),
    c(0.7318481959297978, 0.685455287303686, 0.617854327667774, 0.5193502700643652, 0.37581607444651055, 0.166666666666667),
    c(0.26815180407020217, 0.314544712696314, 0.382145672332226, 0.4806497299356348, 0.6241839255534894, 0.833333333333333),
    c(0.6203703703703703, 0.6296296296296295, 0.6388888888888888, 0.6481481481481481, 0.6574074074074073, 0.6666666666666667),
    c(0.5445496781757002, 0.49195194882958865, 0.42977431049985865, 0.3562719225389128, 0.26938214703294683, 0.16666666666666746),
    c(0.43749999999999994, 0.4166666666666666, 0.3958333333333333, 0.375, 0.35416666666666663, 0.3333333333333333))
) %>%
  unnest_wider(ground_truth_prediction, names_sep="_")

data_time_series <- data_time_series %>%
  left_join(time_series_ground_truth, by=c("func", "direction", "datapoints")) %>%
  mutate(squared_error_1 = (prediction_1-ground_truth_prediction_1)**2,
         squared_error_2 = (prediction_2-ground_truth_prediction_2)**2,
         squared_error_3 = (prediction_3-ground_truth_prediction_3)**2,
         squared_error_4 = (prediction_4-ground_truth_prediction_4)**2,
         squared_error_5 = (prediction_5-ground_truth_prediction_5)**2,
         squared_error_6 = (prediction_6-ground_truth_prediction_6)**2,
         mse = rowMeans(across(starts_with("squared_error_")), na.rm = T)
         )
```

## Bayesian update

### Data

```{r}
data_bayesian_update <- data %>%
  filter(trial_name=="bayesian_update_test_trial", pt_trial==F) %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_bayesian_update)))) %>%
  mutate(ball_split = gsub("[\\[]", "", ball_split),
         ball_split = gsub("\\]", "", ball_split),
         ball_split = gsub(", ", ",", ball_split),
         ball_split = gsub("\'", "", ball_split),
         ball_split = factor(ball_split),
         current_draw = gsub("[\\[]", "", current_draw),
         current_draw = gsub("\\]", "", current_draw),
         current_draw = gsub("\'", "", current_draw),
         current_draw = factor(current_draw),
         past_draws = gsub("[\\[]", "", past_draws),
         past_draws = gsub("\\]", "", past_draws),
         past_draws = gsub("\'", "", past_draws),
         past_draws = gsub(", ", ",", past_draws),
         past_draws = na_if(past_draws, "")) %>%
  # score is incorrect, it's recalculated below
  select(-c(trial_name, block, pt_trial, trial_type, response, score))
```

### Scoring

```{r}
data_bayesian_update <- data_bayesian_update %>%
  mutate(
    combined_draws = ifelse(is.na(past_draws), as.character(current_draw), paste(past_draws, as.character(current_draw), sep = ",")),
    blue_balls = str_count(combined_draws, "blue"),
    red_balls = str_count(combined_draws, "red"),
    
    # Calculate probabilities
    ball_split_numeric = str_split(ball_split, ",") %>% map(~ as.numeric(.x) / 100),
    probability_blue_ball = map_dbl(ball_split_numeric, 2),
    probability_red_ball = map_dbl(ball_split_numeric, 1),
    true_odds_blue_urn = (probability_blue_ball / probability_red_ball) ** (blue_balls - red_balls),
    
    response_slider_modified = case_when(
      response_slider==0 ~ 1,
      response_slider==100 ~ 99,
      TRUE ~ response_slider
    ),
    # Calculate reported probability and true odds based on version
    reported_probability = if_else(
      version == "easy",
      if_else(right_box_majority_color == "blue", response_slider_modified / 100, (100 - response_slider_modified) / 100),
      response_slider_modified / 100
    ),
    true_odds = if_else(
      version == "easy",
      true_odds_blue_urn,
      {
        true_probability_blue_urn = true_odds_blue_urn / (1 + true_odds_blue_urn)
        true_probability_blue_ball = (true_probability_blue_urn * probability_blue_ball) + ((1 - true_probability_blue_urn) * (1 - probability_blue_ball))
        true_probability_blue_ball / (1 - true_probability_blue_ball)
      }
    ),
    # Calculate reported odds
    reported_odds = reported_probability / (1 - reported_probability),
    # Calculate the score
    score = abs(log(reported_odds) - log(true_odds))
  ) %>%
  select(-c(combined_draws, blue_balls, red_balls, ball_split_numeric, probability_blue_ball, probability_red_ball, true_odds_blue_urn, response_slider_modified, reported_probability, true_odds, reported_odds))
```

## Cognitive reflection task

### Data

```{r}
data_cognitive_reflection <- data %>%
  filter(trial_name=="cognitive_reflection_test_trial") %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_cognitive_reflection)))) %>%
  remove_duplicated_tasks_across_waves() %>%
  select(-c(trial_name, block, trial, pt_trial, trial_type, response))
```

### Scoring

```{r}
data_cognitive_reflection <- data_cognitive_reflection %>%
  mutate(crt_correct_1 = crt_1=="5 cents",
         crt_correct_2 = crt_2=="5 minutes",
         crt_correct_3 = crt_3=="47 days",
         crt_correct_4 = crt_4=="4 days",
         crt_correct_5 = crt_5=="29 students",
         crt_correct_6 = crt_6=="20 dollars",
         crt_correct_7 = crt_7=="has lost money.",
         across(starts_with("crt_correct_"), as.integer),
         crt_correct_mean = rowSums(across(starts_with("crt_correct_")), na.rm=T)/7
         )
```

## Number Series task

### Data

```{r}
data_number_series <- data %>%
  filter(trial_name=="number_series_test_trial") %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_number_series)))) %>%
  remove_duplicated_tasks_across_waves(ns_id) %>%
  
  # Regex explanation:
  # (?<=: ') is a positive lookbehind that asserts the match is preceded by ": '"
  # [^']+ matches one or more characters that are not a single quote
  # (?=') is a positive lookahead that asserts the match is followed by a single quote
  mutate(ns_response = str_extract(response, "(?<=: ')[^']+(?=')")) %>%
  
  select(-c(block, trial, pt_trial, response, trial_type))
```

### Scoring

```{r}
data_number_series <- data_number_series %>%
  left_join(tibble(
    ns_id = paste0("NS_", seq(1, 9)),
    correct_answer = c("-2", "28", "64", "29", "8/16", "165", "43", "8900", "12/16")
  ), by="ns_id") %>%
  mutate(correct = as.integer(ns_response==correct_answer))
```

## Coherence Forecasting scale

### Data

```{r}
data_coherence_forecasting_raw <- data %>%
  filter(trial_name=="cfs_test_trial") %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_coherence_forecasting)))) %>%
  remove_duplicated_tasks_across_waves(cfs_id) %>%
  select(-c(trial_name, block, trial, pt_trial, response, trial_type))
```

### Scoring

```{r}
trin <-  function(est1, est2, est3 = 1) {
  trin_unscaled <- 1 - abs(sum(est1, est2) - est3)
  trin_scaled <- scales::rescale(trin_unscaled, c(0,1), c(-1, 1))
  return(trin_scaled)
}

bin <- function(est1, est2, true = 1) {
  est = est1 + est2
  return(1 - abs(est - true))
}

taurank <- function(x, y, z){
  if (sd(as.numeric(c(x, y, z))) == 0) {
    return(0.5)
  }
  
  cor(1:3, rank(as.numeric(c(x, y, z))), method = "kendall") %>%
    scales::rescale(., c(0,1), c(-1, 1))
}

data_coherence_forecasting_scores <- data_coherence_forecasting_raw %>%
  mutate(cfs_id = map(cfs_id, ~ str_extract_all(.x, "\\w+") %>% unlist()),
         cfs_response = map(cfs_response, ~ str_extract_all(.x, "\\d+") %>% unlist())) %>%
  unnest(cols = c(cfs_id, cfs_response)) %>%
  mutate(cfs_response = as.numeric(cfs_response)/100) %>%
  pivot_wider(names_from = cfs_id, values_from = cfs_response,
              values_fn = list(cfs_response = mean), id_cols = c(session_id)) %>%
  rowwise() %>%
  mutate(tri1_abc = trin(trinary_A, trinary_B, (1-trinary_C)),
         # consistent within own responses
         tri1_ab = bin(trinary_A, trinary_B, trinary_AB),
         tri1_bc = bin(trinary_B, trinary_C, trinary_BC),
         # binary items (3)
         bin1 = bin(binary_statepop1, binary_statepop2),
         bin2 = bin(binary_statearea1, binary_statearea2),
         bin3 = bin(binary_gdp1, binary_gdp2),
         # time
         t1 = taurank(time1, time2, time3),
         t2 = taurank(time4, time5, time6),
         # space
         s1 = taurank(space1, space2, space3),
         s2 = taurank(space4, space5, space6),
         ci1 = taurank(confidence_interval1, confidence_interval2, confidence_interval3),
         score_mean = rowMeans(across(tri1_abc:ci1), na.rm=T)
         ) %>%
  ungroup()
```

## Raven matrices

### Data

```{r}
data_raven <- data %>%
  filter(trial_name=="raven_trial", pt_trial==F) %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_raven)))) %>%
  remove_duplicated_tasks_across_waves(pt_trial, block, trial) %>%
  select(-c(trial_name, block, pt_trial, trial_type))
```

### Scores

Due to a typo in the originally shared files by [Matzen et al 2010](https://github.com/sandialabs/Matrices/blob/master/Matzen_et_al_2010_norming_stim.zip), the scoring is redone.

Originally, Matzen et al share this in their codebook (stimulus-correct answer pairs):
A4_1	2
A4_1	1
A4_2	3
A4_3	1

But the keys have a typo. We have changed them in the raven_correct_answer to be:
A4_1	2
A4_2	1
A4_3	3
A4_4	1

We fix this here.

```{r}
data_raven <- data_raven %>%
  # drop incorrectly set variables
  select(-correct_answer, -correct) %>%
  left_join(read_csv("raven_correct_answers.csv", show_col_types=F), by="stimulus") %>%
  # recompute correct scores
  mutate(response = as.numeric(response),
         correct = as.integer(response==correct_answer))
```

## Shipley - vocabulary

### Data

```{r}
data_shipley_vocabulary <- data %>%
  filter(trial_name=="shipley_vocab_test_trial") %>%
  select(any_of(names(c(col_spec_across_tasks)))) %>%
  remove_duplicated_tasks_across_waves() %>%
  select(-c(block, trial, pt_trial, response, trial_type, session_restart_id, trial_name)) %>%
  left_join(select(read_csv("shipley_scores.csv", show_col_types=F), session_id, starts_with("shipley_vocab")),
            by="session_id")
```

## Shipley - abstraction

### Data

```{r}
data_shipley_abstraction <- data %>%
  filter(trial_name=="shipley_abstraction_test_trial") %>%
  select(any_of(names(c(col_spec_across_tasks)))) %>%
  select(-c(block, trial, pt_trial, response, trial_type, session_restart_id, trial_name)) %>%
  left_join(select(read_csv("shipley_scores.csv", show_col_types=F), session_id, starts_with("shipley_abstraction_")),
            by="session_id")
```

## Berlin Numeracy

Even though in the pilot we have more data than in the main experiment, we keep the processing the same (i.e. no RT readily available etc) for consistency's sake.

### Data

```{r}
data_berlin_numeracy <- data %>%
  filter(trial_name=="berlin_numeracy_test_trial") %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_berlin_numeracy)))) %>%
  remove_duplicated_tasks_across_waves() %>%
  
  rename("bn_1"="BNS_1", "bn_2"="BNS_2", "bn_3"="BNS_3", "bn_4"="BNS_4") %>%
  left_join(select(metadata_session, session_id, subject_id), by="session_id") %>%
  select(subject_id, c("bn_1", "bn_2", "bn_3", "bn_4")) %>%
  pivot_longer(cols = -subject_id, names_to = "item_id", values_to = "response") %>%
  left_join(tibble(
    item_id = c("bn_1", "bn_2", "bn_3", "bn_4"),
    correct_response = c("30 out of 50 throws", "25%", "20 out of 70 throws", "50%")
  ), by="item_id") %>%
  mutate(correct = as.integer(response==correct_response))
```

## ADMC

### Data

```{r}
data_admc_raw <- data %>%
  filter(!is.na(admc_id)) %>%
  select(any_of(names(c(col_spec_across_tasks, col_spec_admc)))) %>%
  select(-c(block, trial, pt_trial, trial_type)) %>%
  remove_duplicated_tasks_across_waves(admc_id)
```

### Scoring

```{r}
tmp <- data_admc_raw %>%
  select(session_id, admc_id, admc_response) %>%
  left_join(select(metadata_session, session_id, subject_id), by="session_id") %>%
  mutate(admc_id = strsplit(gsub("\"", "", admc_id), ",", fixed = TRUE),
         admc_response = strsplit(gsub("\"", "", admc_response), ",", fixed = TRUE)) %>%
  unnest(cols = c(admc_id, admc_response)) %>%
  mutate(admc_id = gsub("[\\[]", "", admc_id),
         admc_response = gsub("[\\[]", "", admc_response),
         admc_id = gsub("\\]", "", admc_id),
         admc_response = gsub("\\]", "", admc_response),
         admc_id = gsub("\'", "", admc_id),
         admc_response = gsub("\'", "", admc_response),
         admc_response = as.double(admc_response),
         admc_id = trimws(admc_id)) %>%
  relocate(subject_id)
```

#### Resistance to framing

```{r}
admc_resistance_to_framing_related_frames <- tibble(
  positive_frame = c("rc1_1", "rc1_2", "rc1_3", "rc1_4", "rc1_5", "rc1_6", "rc1_7",
                     "a1_1", "a1_2", "a1_3", "a1_4", "a1_5", "a1_6", "a1_7"),
  negative_frame = c("rc2_5", "rc2_4", "rc2_7", "rc2_2", "rc2_6", "rc2_3", "rc2_1",
                     "a2_6", "a2_5", "a2_3", "a2_1", "a2_7", "a2_2", "a2_4")
)

data_admc_scores <- tmp %>%
  filter(str_starts(admc_id, "rc") | str_starts(admc_id, "a")) %>%
  select(subject_id, admc_id, admc_response) %>%
  
  nest(admc_data = -subject_id) %>%
  mutate(resistance_to_framing = map(admc_data, function(id_data) {
    admc_resistance_to_framing_related_frames <- tibble(
      positive_frame = c("rc1_1", "rc1_2", "rc1_3", "rc1_4", "rc1_5", "rc1_6", "rc1_7",
                         "a1_1", "a1_2", "a1_3", "a1_4", "a1_5", "a1_6", "a1_7"),
      negative_frame = c("rc2_5", "rc2_4", "rc2_7", "rc2_2", "rc2_6", "rc2_3", "rc2_1",
                         "a2_6", "a2_5", "a2_3", "a2_1", "a2_7", "a2_2", "a2_4")
    )
        
    results_per_scale <- list()  # Use a list to store results for each pair
    for (row in 1:nrow(admc_resistance_to_framing_related_frames)) {
      positive_frame_value <- filter(id_data,
                                     admc_id == admc_resistance_to_framing_related_frames[row,]$positive_frame)$admc_response
      negative_frame_value <- filter(id_data,
                                     admc_id == admc_resistance_to_framing_related_frames[row,]$negative_frame)$admc_response
      
      tryCatch({
        if (nrow(id_data) == 14) {
          result <- NA
        } else if (length(positive_frame_value) == 0 | length(negative_frame_value) == 0) {
          result <- NA
        } else if (is.na(positive_frame_value) | is.na(negative_frame_value)) {
          result <- NA
        } else {
          result <- abs(positive_frame_value - negative_frame_value)
        }
        
        # Add the result to the list using frame pair as the key
        frame_pair <- paste(admc_resistance_to_framing_related_frames[row,]$positive_frame,
                            admc_resistance_to_framing_related_frames[row,]$negative_frame,
                            sep = "_vs_")
        results_per_scale[[frame_pair]] <- result
      }, error = function(e) {
        frame_pair <- paste(admc_resistance_to_framing_related_frames[row,]$positive_frame,
                            admc_resistance_to_framing_related_frames[row,]$negative_frame,
                            sep = "_vs_")
        results_per_scale[[frame_pair]] <- NA
      })
    }
    return(as_tibble(results_per_scale))  # Convert list to tibble
  }, .progress=T)) %>%
  unnest(cols = resistance_to_framing) %>%
  select(-admc_data) %>%
  mutate(resistance_to_framing_score_mean = rowMeans(across(c(starts_with("rc1"), starts_with("a1"))), na.rm=T))
```

#### Risk perception

```{r}
calc_admc_rp_consistency_trial <- function(id_data) {
  if (nrow(id_data) != 20) {
    return(NULL)
  }
  # Consistency rules
  rules <- c(id_data$admc_response[id_data$admc_id == "rp_a1"] <= id_data$admc_response[id_data$admc_id == "rp_b1"],
             id_data$admc_response[id_data$admc_id == "rp_a2"] <= id_data$admc_response[id_data$admc_id == "rp_b2"],
             id_data$admc_response[id_data$admc_id == "rp_a3"] <= id_data$admc_response[id_data$admc_id == "rp_b3"],
             id_data$admc_response[id_data$admc_id == "rp_a4"] <= id_data$admc_response[id_data$admc_id == "rp_b4"],
             id_data$admc_response[id_data$admc_id == "rp_a5"] <= id_data$admc_response[id_data$admc_id == "rp_b5"],
             id_data$admc_response[id_data$admc_id == "rp_a6"] <= id_data$admc_response[id_data$admc_id == "rp_b6"],
             id_data$admc_response[id_data$admc_id == "rp_a7"] <= id_data$admc_response[id_data$admc_id == "rp_b7"],
             id_data$admc_response[id_data$admc_id == "rp_a8"] >= id_data$admc_response[id_data$admc_id == "rp_b8"],
             id_data$admc_response[id_data$admc_id == "rp_a9"] <= id_data$admc_response[id_data$admc_id == "rp_b9"],
             id_data$admc_response[id_data$admc_id == "rp_a10"] >= id_data$admc_response[id_data$admc_id == "rp_b10"],
             id_data$admc_response[id_data$admc_id == "rp_a3"] >= id_data$admc_response[id_data$admc_id == "rp_a6"],
             id_data$admc_response[id_data$admc_id == "rp_b3"] >= id_data$admc_response[id_data$admc_id == "rp_b6"],
             id_data$admc_response[id_data$admc_id == "rp_a4"] >= id_data$admc_response[id_data$admc_id == "rp_a7"],
             id_data$admc_response[id_data$admc_id == "rp_b4"] >= id_data$admc_response[id_data$admc_id == "rp_b7"],
             id_data$admc_response[id_data$admc_id == "rp_a2"] <= id_data$admc_response[id_data$admc_id == "rp_a9"],
             id_data$admc_response[id_data$admc_id == "rp_b2"] <= id_data$admc_response[id_data$admc_id == "rp_b9"],
             id_data$admc_response[id_data$admc_id == "rp_a1"] + id_data$admc_response[id_data$admc_id == "rp_a10"] == 1,
             id_data$admc_response[id_data$admc_id == "rp_b1"] + id_data$admc_response[id_data$admc_id == "rp_b10"] == 1,
             id_data$admc_response[id_data$admc_id == "rp_a5"] + id_data$admc_response[id_data$admc_id == "rp_a8"] == 1,
             id_data$admc_response[id_data$admc_id == "rp_b5"] + id_data$admc_response[id_data$admc_id == "rp_b8"] == 1)
  
  rules <- as.integer(rules)
  
  names(rules) <- c(
    "rp_a1_vs_rp_b1",
    "rp_a2_vs_rp_b2",
    "rp_a3_vs_rp_b3",
    "rp_a4_vs_rp_b4",
    "rp_a5_vs_rp_b5",
    "rp_a6_vs_rp_b6",
    "rp_a7_vs_rp_b7",
    "rp_a8_vs_rp_b8",
    "rp_a9_vs_rp_b9",
    "rp_a10_vs_rp_b10",
    "rp_a3_vs_rp_a6",
    "rp_b3_vs_rp_b6",
    "rp_a4_vs_rp_a7",
    "rp_b4_vs_rp_b7",
    "rp_a2_vs_rp_a9",
    "rp_b2_vs_rp_b9",
    "rp_a1_plus_rp_a10_equals_1",
    "rp_b1_plus_rp_b10_equals_1",
    "rp_a5_plus_rp_a8_equals_1",
    "rp_b5_plus_rp_b8_equals_1"
  )

  return(rules)
}

data_admc_scores <- data_admc_scores %>% 
  left_join(tmp %>%
      filter(str_starts(admc_id, "rp")) %>%
      select(subject_id, admc_id, admc_response) %>%
      group_by(subject_id) %>%
      nest() %>%
      mutate(risk_perception = map(data, calc_admc_rp_consistency_trial, .progress=T)) %>%
      ungroup() %>%
      select(-data) %>%
      unnest_wider(risk_perception) %>%
      mutate(risk_perception_prop_consistent = rowSums(across(starts_with("rp_")), na.rm=T)/20),
    by="subject_id")
```

#### Decision rules

```{r warning=FALSE}
data_admc_scores <- data_admc_scores %>% 
  left_join(data_admc_raw %>%
    select(session_id, admc_id, response) %>%
    left_join(select(metadata_session, session_id, subject_id), by="session_id") %>%
    filter(str_starts(admc_id, "dr")) %>%
    select(subject_id, admc_id, response) %>% 
    mutate(across(where(is.factor), as.character)) %>%
    mutate(response = str_extract(response, "\\[.*?\\]")) %>%
    # correct responses
    inner_join(tibble(admc_id=c("dr1", "dr2", "dr3", "dr4", "dr5", "dr6", "dr7", "dr8", "dr8", "dr9", "dr9", "dr9", "dr10", "dr10", "dr10"),
                      admc_correct_response=c("C", "D", "C", "None", "A", "E", "E", "A", "C", "A", "D", "E", "C", "D", "E")),
               by="admc_id") %>%
    rowwise() %>%
    mutate(score = as.integer(str_detect(response, admc_correct_response))) %>%
    ungroup() %>%
    pivot_wider(id_cols=c("subject_id"),
                names_from=c("admc_id", "admc_correct_response"),
                values_from=score) %>%
    mutate(dr_score_mean = rowSums(across(starts_with("dr")), na.rm=T)/15),
  by="subject_id")
```

# Save task datasets

```{r}
datasets_to_save <- list(data_leapfrog_pt_trials_interblock=data_leapfrog_pt_trials_interblock,
                          data_leapfrog=data_leapfrog,
                          data_denominator_neglect=data_denominator_neglect,
                          data_graph_literacy_raw=data_graph_literacy_raw,
                          data_graph_literacy_scores=data_graph_literacy_scores,
                          data_impossible_question=data_impossible_question,
                          data_time_series=data_time_series,
                          data_bayesian_update=data_bayesian_update,
                          data_cognitive_reflection=data_cognitive_reflection,
                          data_number_series=data_number_series,
                          data_coherence_forecasting_raw=data_coherence_forecasting_raw,
                          data_coherence_forecasting_scores=data_coherence_forecasting_scores,
                          data_raven=data_raven,
                          data_shipley_vocabulary=data_shipley_vocabulary,
                          data_shipley_abstraction=data_shipley_abstraction,
                          data_berlin_numeracy=data_berlin_numeracy,
                          data_admc_raw=data_admc_raw,
                          data_admc_scores=data_admc_scores
                         )


folder_path <- file.path("task_datasets")

lapply(names(datasets_to_save), function(name) {
  data_to_save <- datasets_to_save[[name]]
  if (name=="data_time_series") {
    data_to_save <- data_to_save %>%
      mutate(across(c("y_axis_values", "displayed_values"), ~ map_chr(., ~ paste(.x, collapse = ","))))
  }
  write.csv(data_to_save, file.path("task_datasets", paste0(name, ".csv")), row.names = FALSE)
})

save(datasets_to_save, file = file.path("task_datasets", "FPT_datasets.RData"))
```

# Convenience datasests

## summary scores across tasks

### by session

```{r}
summary_scores_all_tasks_by_session <- data %>%
  select(session_id) %>%
  unique() %>%
  
  left_join(select(metadata_session, session_id, subject_id, form, wave), by="session_id") %>%
  
  left_join(data_leapfrog %>%
    group_by(session_id) %>%
    summarise(leapfrog_optimal_choice_mean = mean(optimal_choice, na.rm=T), .groups='drop'),
    by="session_id") %>%
  left_join(data_denominator_neglect %>%
    group_by(session_id, task_version) %>%
    summarise(correct_mean = mean(correct, na.rm=T), .groups='drop') %>%
    pivot_wider(id_cols=session_id, 
                names_from=task_version, names_glue="dn_version_{task_version}_correct_mean", 
                values_from=correct_mean),
    by="session_id") %>%
  left_join(data_graph_literacy_scores %>%
    group_by(session_id) %>%
    summarise(gl_correct_mean = sum(correct, na.rm=T)/13, .groups='drop'),
    by="session_id") %>%
  left_join(data_impossible_question %>%
    group_by(session_id, question_type) %>%
    summarise(sum_correct = sum(correct, na.rm=T),
              calibration = mean(confidence_scaled, na.rm=T) - mean(correct, na.rm=T),
              .groups="drop") %>%
    mutate(mean_correct = if_else(question_type=="IQ", sum_correct/6, sum_correct/24)) %>%
    select(-sum_correct) %>%
    pivot_wider(id_cols=session_id,
                names_from=question_type, names_glue="impossible_question_{question_type}_{.value}",
                values_from=c(mean_correct, calibration)) %>%
    select(-impossible_question_IQ_calibration),
    by="session_id") %>%
  left_join(data_time_series %>%
    group_by(session_id) %>%
    summarise(time_series_mse_mean = mean(mse, na.rm=T), .groups='drop'),
    by="session_id") %>%
  left_join(data_bayesian_update %>%
    group_by(session_id, version) %>%
    summarise(bayesian_update = mean(score, na.rm=T), .groups='drop') %>%
    pivot_wider(id_cols=session_id, names_from=version, names_glue="bayesian_update_{version}_mean", values_from=bayesian_update),
    by="session_id") %>%
  left_join(data_cognitive_reflection %>%
    select(session_id, crt_correct_mean),
    by="session_id") %>%
  left_join(data_number_series %>%
    group_by(session_id) %>%
    summarise(number_series_correct_mean = sum(correct, na.rm=T)/9, .groups='drop'),
    by="session_id") %>%
  left_join(data_coherence_forecasting_scores %>%
    select(session_id, score_mean) %>%
    rename(cfs_score_mean=score_mean),
    by="session_id") %>%
  left_join(data_raven %>%
    group_by(session_id) %>%
    summarise(raven_correct_mean = sum(correct, na.rm=T)/42, .groups='drop'),
    by="session_id") %>%
  left_join(data_shipley_vocabulary %>%
    mutate(shipley_vocab_mean = shipley_vocab_total/40) %>%
    select(session_id, shipley_vocab_mean),
    by="session_id") %>%
  left_join(data_shipley_abstraction %>%
    select(-shipley_abstraction_total) %>%
    pivot_longer(cols = starts_with("shipley_abstraction_"), names_to = "trial_subtrial", values_to = "correct") %>%
    mutate(trial = as.integer(str_extract(trial_subtrial, "(?<=shipley_abstraction_)\\d+(?=_)"))) %>%
    group_by(session_id, trial) %>%
    summarize(trial_score = if_else(all(correct == 1), 1, 0), .groups = "drop") %>%
    group_by(session_id) %>%
    summarize(shipley_abstraction_score_mean = sum(trial_score,na.rm=T)/25, .groups = "drop"),
    by="session_id") %>%
  left_join(data_berlin_numeracy %>%
    group_by(subject_id) %>%
    summarize(berlin_numeracy_score_sum = sum(correct,na.rm=T)/4, .groups="drop"),
    by="subject_id") %>%
  left_join(data_admc_scores %>%
    select(subject_id, resistance_to_framing_score_mean, risk_perception_prop_consistent, dr_score_mean),
    by="subject_id")
  
write.csv(summary_scores_all_tasks_by_session,
          file.path("convenience_datasets", "summary_scores_all_tasks_by_session.csv"), 
          row.names=F)
```

### by subject

```{r}
summary_scores_all_tasks_by_subject_id <- summary_scores_all_tasks_by_session %>%
  group_by(subject_id) %>%
  summarise(across(where(is.numeric), ~mean(., na.rm=T)), .groups='drop')

write.csv(summary_scores_all_tasks_by_subject_id,
          file.path("convenience_datasets", "summary_scores_all_tasks_by_subject_id.csv"), 
          row.names=F)
```

